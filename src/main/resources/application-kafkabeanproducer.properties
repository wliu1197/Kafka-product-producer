server.port=8080
kafka.producer.bootstrap-servers=localhost:9092,localhost:9094
kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
kafka.producer.value-serializer=org.springframework.kafka.support.serializer.JsonSerializer

#producer will wait for all broker servers to return acknowledgements back
#spring.kafka.producer.acks=all
#spring.kafka.producer.acks=1 (only wait for leader broker to send acks back)
# for idempotence to work this must be all
kafka.producer.acks=all

# by defualt is max integer value and in order for idempotence to work you need to have this >0
#when message failed to be stored in kafka broker it will retry max 10 times
kafka.producer.retries=10

#wait time between each retry
kafka.producer.properties.retry.backoff.ms=1000

#producer wait 2 mins(default is 2 mins) for acks then time out
kafka.producer.properties.delivery.timeout.ms=120000
#delivery.timeout.ms>=linger.ms+request.timeout.ms
#producer wait 0 ms for buffer data before sending message to broker (by default is 0ms)
kafka.producer.properties.linger.ms=0
#producer wait for each ack response (by defualt is 30000ms)
kafka.producer.properties.request.timeout.ms=30000

# By setting your producer as idempotence
# To avoid duplicate messages stored in broker partitions
# when acknowledge send back to producer if producer has network issue didn't receive ack
# it will retry to send again same message in this time the same message won't be stored
# broker will return ack again directly
# this by default is enabled
kafka.producer.properties.enable.idempotence=true

# how many requests can be made in parallel to any partition
# by defualt is 5 and in order for idempotence to work you need to have this <=5
kafka.producer.properties.max.in.flight.request.per.connection=5

# apply kafka transaction
# enable Kafka transaction when produce messages to topics
# if we are running microservice in AWS ECS we have multiple instances then for each instance it needs unique transaction-id-prefix
# ${random.value} will generate unique value in the transaction-id-prefix and Kafka will auto append id after prefix for each transaction
# e.g: transfer-service-${random.value}-0, transfer-service-${random.value}-1 etc...
kafka.producer.transaction-id-prefix=transfer-service-${random.value}-
logging.level.org.springframework.kafka.transaction=debug
logging.level.org.springframework.transaction=trace
logging.level.org.springframework.orm.jpa.JpaTransactionManager=debug
logging.level.org.springframework.kafka.transaction.KafkaTransactionManager=debug
logging.level.org.apache.kafka.clients.producer.internals.TransactionManager=debug

#--------------------- MYSQL DB ---------------------#
spring.jpa.show-sql=true
spring.jpa.defer-datasource-initialization=true

spring.datasource.url=${SPRING_DATASOURCE_URL:jdbc:mysql://localhost:3306/kafka-demo-database}
spring.datasource.username=${SPRING_DATASOURCE_USERNAME:root}
spring.datasource.password=${SPRING_DATASOURCE_PASSWORD:dummypassword}

#
# For MySql spring entity mapping won't auto generate database table for us
# However if we need we can configure by suing spring.jpa.hibernate.ddl-auto=update
# none, create, create-drop(for testing create and remove), validate, and update (update column without remove data)
# WARNING: dangerous to use this as if we have connect to an existing db
# In production, it's often highly recommended you use none or simply don't specify this property.
#
spring.jpa.hibernate.ddl-auto=update